/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.tantivy4spark.bloom

import org.slf4j.LoggerFactory
import com.google.common.hash.{BloomFilter => GuavaBloomFilter, Funnels}
import com.google.common.hash.Hashing
import java.io.{ByteArrayInputStream, ByteArrayOutputStream}
import java.nio.charset.StandardCharsets
import scala.collection.mutable

/**
 * Efficient bloom filter implementation optimized for text search acceleration.
 * 
 * Design considerations:
 * - Space-efficient: Uses compressed bit arrays
 * - False positive rate: Configurable (default ~1%)
 * - Hash functions: Uses SHA-256 with multiple salts for good distribution
 * - Serialization: Compact binary format for S3 storage optimization
 */
class BloomFilter(
    val expectedItems: Int,
    val falsePositiveRate: Double = 0.01
) {
  
  
  private val logger = LoggerFactory.getLogger(classOf[BloomFilter])
  
  // Calculate optimal bloom filter parameters
  val bitArraySize: Int = calculateOptimalBitArraySize(expectedItems, falsePositiveRate)
  val numHashFunctions: Int = calculateOptimalHashFunctions(bitArraySize, expectedItems)
  
  private[bloom] val bitArray = new JavaBitSet(bitArraySize)
  private var itemCount = 0
  
  // Method to set item count for deserialization
  private[bloom] def setItemCount(count: Int): Unit = {
    itemCount = count
  }
  
  /**
   * Add a token to the bloom filter
   */
  def add(token: String): Unit = {
    val hashes = computeHashes(token)
    hashes.foreach { hash =>
      val index = Math.abs(hash % bitArraySize)
      bitArray.set(index)
    }
    itemCount += 1
  }
  
  /**
   * Test if a token might be present in the bloom filter
   * Returns false if definitely not present, true if might be present
   */
  def mightContain(token: String): Boolean = {
    val hashes = computeHashes(token)
    hashes.forall { hash =>
      val index = Math.abs(hash % bitArraySize)
      bitArray.get(index)
    }
  }
  
  /**
   * Get current estimated false positive rate based on actual usage
   */
  def getCurrentFalsePositiveRate: Double = {
    val bitsSet = bitArray.cardinality()
    if (bitsSet == 0) return 0.0
    
    val ratio = bitsSet.toDouble / bitArraySize
    Math.pow(ratio, numHashFunctions)
  }
  
  /**
   * Serialize bloom filter to compact byte array for storage
   */
  def serialize(): Array[Byte] = {
    val bitArrayBytes = bitArray.toByteArray
    // Store: bitArraySize, numHashFunctions, itemCount, falsePositiveRate, bitArrayBytesLength, bitArrayBytes
    val buffer = ByteBuffer.allocate(4 + 4 + 4 + 8 + 4 + bitArrayBytes.length)
    buffer.putInt(bitArraySize)
    buffer.putInt(numHashFunctions) 
    buffer.putInt(itemCount)
    buffer.putDouble(falsePositiveRate)
    buffer.putInt(bitArrayBytes.length)
    buffer.put(bitArrayBytes)
    buffer.array()
  }
  
  /**
   * Compute multiple hash values for a token using SHA-256 with different salts
   */
  private def computeHashes(token: String): Array[Int] = {
    val digest = MessageDigest.getInstance("SHA-256")
    val baseHash = digest.digest(token.getBytes("UTF-8"))
    
    (0 until numHashFunctions).map { i =>
      // Create different hash functions by adding salt
      val saltedHash = digest.digest((token + s"_salt_$i").getBytes("UTF-8"))
      ByteBuffer.wrap(saltedHash).getInt()
    }.toArray
  }
  
  private def calculateOptimalBitArraySize(n: Int, p: Double): Int = {
    BloomFilter.calculateOptimalBitArraySize(n, p)
  }
  
  private def calculateOptimalHashFunctions(m: Int, n: Int): Int = {
    BloomFilter.calculateOptimalHashFunctions(m, n)
  }
  
  def getStats: Map[String, Any] = Map(
    "bitArraySize" -> bitArraySize,
    "numHashFunctions" -> numHashFunctions,
    "itemCount" -> itemCount,
    "targetFalsePositiveRate" -> falsePositiveRate,
    "currentFalsePositiveRate" -> getCurrentFalsePositiveRate,
    "bitsSet" -> bitArray.cardinality(),
    "utilization" -> (bitArray.cardinality().toDouble / bitArraySize)
  )
}

object BloomFilter {
  private val logger = LoggerFactory.getLogger(BloomFilter.getClass)
  
  def calculateOptimalBitArraySize(n: Int, p: Double): Int = {
    val size = -(n * Math.log(p)) / (Math.log(2) * Math.log(2))
    Math.max(size.toInt, 64) // Minimum size of 64 bits
  }
  
  def calculateOptimalHashFunctions(m: Int, n: Int): Int = {
    val k = (m.toDouble / n) * Math.log(2)
    Math.max(k.toInt, 1) // At least 1 hash function
  }
  
  /**
   * Deserialize bloom filter from byte array
   */
  def deserialize(data: Array[Byte]): BloomFilter = {
    try {
      val buffer = ByteBuffer.wrap(data)
      val originalBitArraySize = buffer.getInt()
      val originalNumHashFunctions = buffer.getInt()
      val itemCount = buffer.getInt()
      val falsePositiveRate = buffer.getDouble()
      val bitArrayBytesLength = buffer.getInt() // Read the actual length
      
      val bitArrayBytes = new Array[Byte](bitArrayBytesLength)
      buffer.get(bitArrayBytes)
      
      // Use the factory method to create a properly deserialized filter
      val filter = BloomFilter.createForDeserialization(
        originalBitArraySize,
        originalNumHashFunctions, 
        itemCount,
        falsePositiveRate,
        bitArrayBytes
      )
      
      logger.info(s"Deserialized bloom filter: size=$originalBitArraySize, functions=$originalNumHashFunctions, items=$itemCount")
      filter
    } catch {
      case e: Exception =>
        logger.error(s"Failed to deserialize bloom filter from ${data.length} bytes", e)
        // Return a minimal working filter as fallback
        new BloomFilter(100, 0.01)
    }
  }
  
  /**
   * Create bloom filter from collection of tokens
   */
  def fromTokens(tokens: Iterable[String], falsePositiveRate: Double = 0.01): BloomFilter = {
    val filter = new BloomFilter(tokens.size, falsePositiveRate)
    tokens.foreach(filter.add)
    filter
  }
  
  /**
   * Create a bloom filter for deserialization with exact parameters
   */
  private[bloom] def createForDeserialization(
      originalBitArraySize: Int, 
      originalNumHashFunctions: Int,
      itemCount: Int,
      falsePositiveRate: Double,
      bitArrayBytes: Array[Byte]
  ): BloomFilter = {
    // Create a custom deserialized bloom filter with exact original parameters
    val filter = new BloomFilter(1, falsePositiveRate) {
      // Override the calculated parameters with the original ones
      override val bitArraySize: Int = originalBitArraySize
      override val numHashFunctions: Int = originalNumHashFunctions
      
      // Create a new bit array with the original size
      override private[bloom] val bitArray = new JavaBitSet(originalBitArraySize)
    }
    
    // Restore the bit array from the serialized bytes
    val deserializedBitArray = JavaBitSet.valueOf(bitArrayBytes)
    
    // Copy ALL bits from the deserialized bit array
    var i = deserializedBitArray.nextSetBit(0)
    while (i >= 0) {
      filter.bitArray.set(i)
      i = deserializedBitArray.nextSetBit(i + 1)
    }
    
    filter.setItemCount(itemCount)
    logger.info(s"Created exact deserialized filter: size=$originalBitArraySize, functions=$originalNumHashFunctions, items=$itemCount")
    filter
  }
}

/**
 * Manages bloom filters for text columns with intelligent tokenization
 * and S3-optimized storage patterns.
 */
class BloomFilterManager {
  private val logger = LoggerFactory.getLogger(classOf[BloomFilterManager])
  private val textTokenizer = new TextTokenizer()
  
  /**
   * Create bloom filters for text columns in a dataset
   * Returns Map[columnName -> BloomFilter]
   */
  def createBloomFilters(
      textColumns: Map[String, Iterable[String]], 
      falsePositiveRate: Double = 0.01
  ): Map[String, BloomFilter] = {
    
    textColumns.map { case (columnName, values) =>
      logger.info(s"Creating bloom filter for column '$columnName' with ${values.size} values")
      
      // Tokenize all text values for this column
      val allTokens = values.flatMap { text =>
        if (text != null && text.nonEmpty) {
          textTokenizer.tokenize(text)
        } else {
          Seq.empty
        }
      }
      
      val uniqueTokens = allTokens.toSet
      logger.info(s"Column '$columnName': ${values.size} values -> ${allTokens.size} tokens -> ${uniqueTokens.size} unique tokens")
      
      val filter = BloomFilter.fromTokens(uniqueTokens, falsePositiveRate)
      logger.info(s"Created bloom filter for '$columnName': ${filter.getStats}")
      
      columnName -> filter
    }.toMap
  }
  
  /**
   * Test if any of the search terms might be present in the given bloom filters
   * Uses smart query expansion and tokenization
   */
  def mightContainAnyTerm(
      bloomFilters: Map[String, BloomFilter],
      searchTerms: Iterable[String]
  ): Boolean = {
    
    if (bloomFilters.isEmpty || searchTerms.isEmpty) {
      return true // Conservative approach - include file if no filters or terms
    }
    
    // Tokenize search terms
    val searchTokens = searchTerms.flatMap(textTokenizer.tokenize).toSet
    
    if (searchTokens.isEmpty) {
      return true
    }
    
    // Check if any bloom filter might contain any of the search tokens
    bloomFilters.values.exists { filter =>
      searchTokens.exists(filter.mightContain)
    }
  }
  
  /**
   * Advanced search with column-specific filtering
   */
  def evaluateColumnSearch(
      bloomFilters: Map[String, BloomFilter],
      columnSearches: Map[String, Iterable[String]]
  ): Boolean = {
    
    if (bloomFilters.isEmpty || columnSearches.isEmpty) {
      return true
    }
    
    // For each column search, check if the bloom filter might contain the terms
    columnSearches.forall { case (columnName, searchTerms) =>
      bloomFilters.get(columnName) match {
        case Some(filter) =>
          val searchTokens = searchTerms.flatMap(textTokenizer.tokenize)
          searchTokens.exists(filter.mightContain)
        case None =>
          true // No bloom filter for this column - include conservatively
      }
    }
  }
  
  /**
   * Get detailed statistics for all bloom filters
   */
  def getDetailedStats(bloomFilters: Map[String, BloomFilter]): Map[String, Map[String, Any]] = {
    bloomFilters.map { case (column, filter) =>
      column -> filter.getStats
    }
  }
}

/**
 * Splunk-style text tokenizer that handles various text formats intelligently
 */
class TextTokenizer {
  private val logger = LoggerFactory.getLogger(classOf[TextTokenizer])
  
  // Configuration for tokenization
  private val minTokenLength = 2
  private val maxTokenLength = 50
  private val includeNgrams = true
  private val ngramSize = 3
  
  /**
   * Tokenize text into searchable terms using Splunk-style approach:
   * - Word boundaries (alphanumeric sequences)
   * - Common separators (dots, underscores, hyphens)
   * - N-grams for partial matching
   * - Case insensitive
   */
  def tokenize(text: String): Set[String] = {
    if (text == null || text.isEmpty) {
      return Set.empty
    }
    
    val normalized = text.toLowerCase.trim
    val tokens = mutable.Set[String]()
    
    // 1. Extract word sequences (including Unicode letters and digits)
    val wordPattern = "[\\p{L}\\p{N}]+".r // Unicode letters and numbers
    val words = wordPattern.findAllIn(normalized).toSeq
    tokens ++= words.filter(w => w.length >= minTokenLength && w.length <= maxTokenLength)
    
    // 2. Extract terms separated by common delimiters
    val delimiterTokens = normalized.split("[^\\p{L}\\p{N}]+").filter(_.nonEmpty)
    tokens ++= delimiterTokens.filter(t => t.length >= minTokenLength && t.length <= maxTokenLength)
    
    // 3. Create n-grams for partial matching (like Splunk's substring search)
    if (includeNgrams && normalized.length >= ngramSize) {
      val ngrams = for {
        i <- 0 to normalized.length - ngramSize
        ngram = normalized.substring(i, i + ngramSize)
        if ngram.matches("[\\p{L}\\p{N}]+") // Only letter/number n-grams
      } yield ngram
      
      tokens ++= ngrams
    }
    
    // 4. Add the full normalized text as a token (for exact phrase matching)
    if (normalized.length >= minTokenLength && normalized.length <= maxTokenLength) {
      tokens += normalized
    }
    
    val result = tokens.toSet
    logger.debug(s"Tokenized '$text' -> ${result.size} tokens: ${result.take(10)}")
    result
  }
  
  /**
   * Extract searchable terms from a query string
   * Handles quoted phrases, boolean operators, wildcards
   */
  def extractSearchTerms(query: String): Set[String] = {
    if (query == null || query.isEmpty) {
      return Set.empty
    }
    
    val terms = mutable.Set[String]()
    
    // Handle quoted phrases
    val quotedPattern = "\"([^\"]+)\"".r
    val quotes = quotedPattern.findAllMatchIn(query).map(_.group(1)).toSeq
    terms ++= quotes.flatMap(tokenize)
    
    // Remove quoted parts and process the rest
    val withoutQuotes = quotedPattern.replaceAllIn(query, " ")
    
    // Extract individual terms (ignore common boolean operators)
    val booleanOperators = Set("and", "or", "not", "AND", "OR", "NOT")
    val termPattern = "[\\p{L}\\p{N}]+".r
    val individualTerms = termPattern.findAllIn(withoutQuotes)
      .filter(t => !booleanOperators.contains(t) && t.length >= minTokenLength)
      .toSeq
    
    terms ++= individualTerms.flatMap(tokenize)
    
    terms.toSet
  }
}