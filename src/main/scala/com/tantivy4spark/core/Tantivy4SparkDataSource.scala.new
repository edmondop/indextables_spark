/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


package com.tantivy4spark.core

import org.apache.spark.sql.connector.catalog.{SupportsRead, SupportsWrite, TableCapability}
import org.apache.spark.sql.connector.read.{Batch, Scan, ScanBuilder}
import org.apache.spark.sql.connector.write.{LogicalWriteInfo, WriteBuilder}
import org.apache.spark.sql.sources.{BaseRelation, CreatableRelationProvider, DataSourceRegister, RelationProvider, TableScan, PrunedFilteredScan, CatalystScan, Filter}
import org.apache.spark.sql.types.{StructType, TimestampType, DateType, LongType, StringType, DoubleType, FloatType, IntegerType, BooleanType}
import org.apache.spark.sql.connector.catalog.{Table, TableProvider}
import org.apache.spark.sql.util.CaseInsensitiveStringMap
import org.apache.spark.sql.{DataFrame, SQLContext, SaveMode}
import org.slf4j.LoggerFactory
import java.util.zip.ZipInputStream
import java.io.ByteArrayInputStream
import java.nio.file.Files
import java.util
import scala.util.Using

case class Tantivy4SparkRelation(
  path: String, 
  sqlContext: SQLContext, 
  options: Map[String, String] = Map.empty
) extends BaseRelation with PrunedFilteredScan with CatalystScan with TableScan {

  def extractZipToDirectory(zipData: Array[Byte], targetDir: java.nio.file.Path): Unit = {
    Using.resources(
      new ZipInputStream(new ByteArrayInputStream(zipData))
    ) { zipStream =>
      var entry = zipStream.getNextEntry
      while (entry != null) {
        val filePath = targetDir.resolve(entry.getName)
        if (entry.isDirectory) {
          Files.createDirectories(filePath)
        } else {
          Files.createDirectories(filePath.getParent)
          Files.write(filePath, zipStream.readAllBytes())
        }
        entry = zipStream.getNextEntry
      }
    }
  }

  // processFile should not be called without metadata from transaction log - must be called through partition reader with AddAction metadata
  def processFile(
    filePath: String, 
    serializableSchema: StructType, 
    hadoopConfProps: Map[String, String],
    filters: Array[Filter] = Array.empty,
    limit: Option[Int] = None
  ): Iterator[org.apache.spark.sql.Row] = {
    // Create local logger for executor to avoid serialization issues
    val executorLogger = LoggerFactory.getLogger(Tantivy4SparkRelation.getClass)
    
    // Recreate Hadoop configuration in executor context
    val localHadoopConf = new org.apache.hadoop.conf.Configuration()
    hadoopConfProps.foreach { case (key, value) =>
      localHadoopConf.set(key, value)
    }
    
    // DEBUG: Print all tantivy4spark configurations received in executor
    val tantivyConfigs = hadoopConfProps.filter(_._1.startsWith("spark.tantivy4spark."))
    if (executorLogger.isDebugEnabled) {
      executorLogger.debug(s"processFile received ${hadoopConfProps.size} total config properties")
      executorLogger.debug(s"processFile found ${tantivyConfigs.size} tantivy4spark configs:")
      tantivyConfigs.foreach { case (key, value) =>
        val displayValue = if (key.contains("secret") || key.contains("session")) "***" else value
        executorLogger.debug(s"   $key = $displayValue")
      }
    }
    
    // Extract cache configuration with session token support from Hadoop props
    val cacheConfig = com.tantivy4spark.storage.SplitCacheConfig(
      cacheName = {
        val configName = hadoopConfProps.getOrElse("spark.tantivy4spark.cache.name", "")
        if (configName.trim.nonEmpty) {
          configName.trim
        } else {
          // Use file path as cache name for table-specific caching
          val tablePath = new org.apache.hadoop.fs.Path(filePath).getParent.toString
          s"tantivy4spark-${tablePath.replaceAll("[^a-zA-Z0-9]", "_")}"
        }
      },
      maxCacheSize = hadoopConfProps.getOrElse("spark.tantivy4spark.cache.maxSize", "200000000").toLong,
      maxConcurrentLoads = hadoopConfProps.getOrElse("spark.tantivy4spark.cache.maxConcurrentLoads", "8").toInt,
      enableQueryCache = hadoopConfProps.getOrElse("spark.tantivy4spark.cache.queryCache", "true").toBoolean,
      // AWS configuration with session token support (handle both camelCase and lowercase keys)
      awsAccessKey = hadoopConfProps.get("spark.tantivy4spark.aws.accessKey").orElse(hadoopConfProps.get("spark.tantivy4spark.aws.accesskey")),
      awsSecretKey = hadoopConfProps.get("spark.tantivy4spark.aws.secretKey").orElse(hadoopConfProps.get("spark.tantivy4spark.aws.secretkey")),
      awsSessionToken = hadoopConfProps.get("spark.tantivy4spark.aws.sessionToken").orElse(hadoopConfProps.get("spark.tantivy4spark.aws.sessiontoken")),
      awsRegion = hadoopConfProps.get("spark.tantivy4spark.aws.region"),
      awsEndpoint = hadoopConfProps.get("spark.tantivy4spark.s3.endpoint"),
      // Azure configuration
      azureAccountName = hadoopConfProps.get("spark.tantivy4spark.azure.accountName"),
      azureAccountKey = hadoopConfProps.get("spark.tantivy4spark.azure.accountKey"),
      azureConnectionString = hadoopConfProps.get("spark.tantivy4spark.azure.connectionString"),
      azureEndpoint = hadoopConfProps.get("spark.tantivy4spark.azure.endpoint"),
      // GCP configuration
      gcpProjectId = hadoopConfProps.get("spark.tantivy4spark.gcp.projectId"),
      gcpServiceAccountKey = hadoopConfProps.get("spark.tantivy4spark.gcp.serviceAccountKey"),
      gcpCredentialsFile = hadoopConfProps.get("spark.tantivy4spark.gcp.credentialsFile"),
      gcpEndpoint = hadoopConfProps.get("spark.tantivy4spark.gcp.endpoint")
    )
    
    // Use SplitSearchEngine to read split files directly
    val rows = scala.collection.mutable.ListBuffer[org.apache.spark.sql.Row]()
    
    // ERROR: processFile should not be called without metadata from transaction log
    val normalizedPath = filePath
    executorLogger.error(s"❌ ERROR: processFile() called for split without metadata: $normalizedPath")
    executorLogger.error(s"❌ Split files should always be read through transaction log with proper metadata")
    executorLogger.error(s"❌ Use the partition-aware reader that has access to AddAction metadata instead")
    throw new IllegalStateException(s"Split file access requires metadata from transaction log. File: $normalizedPath. Use transaction log reader instead of direct file access.")
  }

  // Enhanced processFile that can handle both Spark filters and custom IndexQuery filters
  def processFileWithCustomFilters(
      filePath: String, 
      serializableSchema: StructType, 
      hadoopConfProps: Map[String, String],
      sparkFilters: Array[Filter] = Array.empty,
      customFilters: Array[Any] = Array.empty,
      limit: Option[Int] = None
  ): Iterator[org.apache.spark.sql.Row] = {
    import com.tantivy4spark.filters.{IndexQueryFilter, IndexQueryAllFilter}
    
    // Create local logger for executor to avoid serialization issues
    val executorLogger = LoggerFactory.getLogger(Tantivy4SparkRelation.getClass)
    
    // Recreate Hadoop configuration in executor context
    val localHadoopConf = new org.apache.hadoop.conf.Configuration()
    hadoopConfProps.foreach { case (key, value) =>
      localHadoopConf.set(key, value)
    }
    
    // DEBUG: Print all tantivy4spark configurations received in executor
    val tantivyConfigs = hadoopConfProps.filter(_._1.startsWith("spark.tantivy4spark."))
    if (executorLogger.isDebugEnabled) {
      executorLogger.debug(s"processFileWithCustomFilters received ${hadoopConfProps.size} total config properties")
      executorLogger.debug(s"processFileWithCustomFilters found ${tantivyConfigs.size} tantivy4spark configs:")
      tantivyConfigs.foreach { case (key, value) =>
        val displayValue = if (key.contains("secret") || key.contains("session")) "***" else value
        executorLogger.debug(s"   $key = $displayValue")
      }
    }
    
    // Extract cache configuration with session token support from Hadoop props
    val cacheConfig = com.tantivy4spark.storage.SplitCacheConfig(
      cacheName = {
        val configName = hadoopConfProps.getOrElse("spark.tantivy4spark.cache.name", "")
        if (configName.trim.nonEmpty) {
          configName.trim
        } else {
          // Use file path as cache name for table-specific caching
          val tablePath = new org.apache.hadoop.fs.Path(filePath).getParent.toString
          s"tantivy4spark-${tablePath.replaceAll("[^a-zA-Z0-9]", "_")}"
        }
      },
      maxCacheSize = hadoopConfProps.getOrElse("spark.tantivy4spark.cache.maxSize", "200000000").toLong,
      maxConcurrentLoads = hadoopConfProps.getOrElse("spark.tantivy4spark.cache.maxConcurrentLoads", "8").toInt,
      enableQueryCache = hadoopConfProps.getOrElse("spark.tantivy4spark.cache.queryCache", "true").toBoolean,
      // AWS configuration with session token support (handle both camelCase and lowercase keys)
      awsAccessKey = hadoopConfProps.get("spark.tantivy4spark.aws.accessKey").orElse(hadoopConfProps.get("spark.tantivy4spark.aws.accesskey")),
      awsSecretKey = hadoopConfProps.get("spark.tantivy4spark.aws.secretKey").orElse(hadoopConfProps.get("spark.tantivy4spark.aws.secretkey")),
      awsSessionToken = hadoopConfProps.get("spark.tantivy4spark.aws.sessionToken").orElse(hadoopConfProps.get("spark.tantivy4spark.aws.sessiontoken")),
      awsRegion = hadoopConfProps.get("spark.tantivy4spark.aws.region"),
      awsEndpoint = hadoopConfProps.get("spark.tantivy4spark.s3.endpoint"),
      // Azure configuration
      azureAccountName = hadoopConfProps.get("spark.tantivy4spark.azure.accountName"),
      azureAccountKey = hadoopConfProps.get("spark.tantivy4spark.azure.accountKey"),
      azureConnectionString = hadoopConfProps.get("spark.tantivy4spark.azure.connectionString"),
      azureEndpoint = hadoopConfProps.get("spark.tantivy4spark.azure.endpoint"),
      // GCP configuration
      gcpProjectId = hadoopConfProps.get("spark.tantivy4spark.gcp.projectId"),
      gcpServiceAccountKey = hadoopConfProps.get("spark.tantivy4spark.gcp.serviceAccountKey"),
      gcpCredentialsFile = hadoopConfProps.get("spark.tantivy4spark.gcp.credentialsFile"),
      gcpEndpoint = hadoopConfProps.get("spark.tantivy4spark.gcp.endpoint")
    )
    
    // Use SplitSearchEngine to read split files directly
    val rows = scala.collection.mutable.ListBuffer[org.apache.spark.sql.Row]()
    
    // ERROR: processFileWithCustomFilters should not be called without metadata from transaction log
    val normalizedPath = filePath
    executorLogger.error(s"❌ ERROR: processFileWithCustomFilters() called for split without metadata: $normalizedPath")
    executorLogger.error(s"❌ Split files should always be read through transaction log with proper metadata")
    executorLogger.error(s"❌ Use the partition-aware reader that has access to AddAction metadata instead")
    throw new IllegalStateException(s"Split file access requires metadata from transaction log. File: $normalizedPath. Use transaction log reader instead of direct file access.")
  }
}

class Tantivy4SparkDataSource extends DataSourceRegister with RelationProvider with CreatableRelationProvider {
  @transient private lazy val logger = LoggerFactory.getLogger(classOf[Tantivy4SparkDataSource])
  
  override def shortName(): String = "tantivy4spark"

  override def createRelation(
      sqlContext: SQLContext,
      parameters: Map[String, String]
  ): BaseRelation = {
    // For reads, create a relation that can handle queries
    val path = parameters.getOrElse("path", throw new IllegalArgumentException("Path is required"))
    new Tantivy4SparkRelation(path, sqlContext, parameters)
  }

  override def createRelation(
      sqlContext: SQLContext,
      mode: SaveMode,
      parameters: Map[String, String],
      data: DataFrame
  ): BaseRelation = {
    // For writes, delegate to the V2 TableProvider approach
    val path = parameters.getOrElse("path", throw new IllegalArgumentException("Path is required"))
    
    // Create a V2 table instance and use its writeBuilder
    val v2DataSource = new Tantivy4SparkTableProvider()
    val options = new CaseInsensitiveStringMap(parameters.asJava)
    val table = v2DataSource.getTable(
      org.apache.spark.sql.types.StructType(Seq.empty),
      Seq.empty,
      options.asJava.asInstanceOf[java.util.Map[String, String]]
    ).asInstanceOf[Tantivy4SparkTable]
    
    // Create write info
    val writeInfo = new LogicalWriteInfo {
      override def queryId(): String = java.util.UUID.randomUUID().toString
      override def schema(): StructType = data.schema
      override def options(): CaseInsensitiveStringMap = options
    }
    
    val writeBuilder = table.newWriteBuilder(writeInfo)
    val batchWrite = writeBuilder.buildForBatch()
    
    // Execute the write using Spark's internal batch write mechanism
    import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils
    DataSourceV2Utils.writeWithMetrics(
      data.queryExecution.executedPlan,
      batchWrite,
      org.apache.spark.sql.execution.metric.SQLMetrics.createMetricsForDataSourceWrite()
    )
    
    // Return the relation for the written data
    new Tantivy4SparkRelation(path, sqlContext, parameters)
  }
}