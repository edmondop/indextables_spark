# CLAUDE.md

**Tantivy4Spark** is a high-performance Spark DataSource implementing fast full-text search using Tantivy via tantivy4java. It runs embedded in Spark executors without server-side components.

## Key Features
- **Split-based architecture**: Write-only indexes with QuickwitSplit format
- **Transaction log**: Delta Lake-style with atomic operations  
- **IndexQuery operators**: Native Tantivy syntax (`content indexquery 'query'` and `_indexall indexquery 'query'`)
- **Optimized writes**: Automatic split sizing with adaptive shuffle
- **S3-optimized storage**: Intelligent caching and session token support
- **Schema-aware filtering**: Field validation prevents native crashes
- **100% test coverage**: 179 tests passing, 0 failing, 73 V2 tests temporarily ignored

## Build & Test
```bash
mvn clean compile  # Build
mvn test          # Run tests  
```

## Configuration
Key settings with defaults:
- `spark.tantivy4spark.indexWriter.heapSize`: `100000000` (100MB)
- `spark.tantivy4spark.indexWriter.batchSize`: `10000` documents
- `spark.tantivy4spark.indexWriter.threads`: `2`
- `spark.tantivy4spark.cache.maxSize`: `200000000` (200MB)
- `spark.tantivy4spark.optimizeWrite.targetRecordsPerSplit`: `1000000`

## Usage Examples

### Write
```scala
// Basic write
df.write.format("tantivy4spark").save("s3://bucket/path")

// With custom configuration
df.write.format("tantivy4spark")
  .option("spark.tantivy4spark.indexWriter.batchSize", "20000")
  .option("targetRecordsPerSplit", "500000")
  .save("s3://bucket/path")
```

### Read & Search
```scala
val df = spark.read.format("tantivy4spark").load("s3://bucket/path")

// Standard DataFrame operations
df.filter($"title".contains("Spark")).show()

// Native Tantivy queries
df.filter($"content" indexquery "machine learning AND spark").show()

// Cross-field search
df.filter($"_indexall" indexquery "apache OR python").show()
```

### SQL
```sql
-- Register extensions (if using SQL)
spark.sparkSession.extensions.add("com.tantivy4spark.extensions.Tantivy4SparkExtensions")

-- Native queries
SELECT * FROM documents WHERE content indexquery 'AI AND (neural OR deep)';
SELECT * FROM documents WHERE _indexall indexquery 'spark AND sql';
```

## Schema Support
**Supported**: String (text), Integer/Long (i64), Float/Double (f64), Boolean (i64), Date (date), Timestamp (i64), Binary (bytes)
**Unsupported**: Arrays, Maps, Structs (throws UnsupportedOperationException)

## Architecture
- **File format**: `*.split` files with UUID naming
- **Transaction log**: `_transaction_log/` directory (Delta Lake compatible)
- **Batch processing**: Uses tantivy4java's BatchDocumentBuilder
- **Caching**: JVM-wide SplitCacheManager with host-based locality
- **Storage**: S3OptimizedReader for S3, StandardFileReader for local/HDFS

## Implementation Status
- âœ… **Core features**: Transaction log, optimized writes, IndexQuery operators
- âœ… **Production ready**: IndexQuery (49/49 tests), IndexQueryAll (44/44 tests)  
- ðŸš§ **V2 DataSource**: Core functionality complete, 73 tests temporarily disabled
- **Next**: Complete V2 integration, resolve date filtering edge cases

## Important Notes
- **tantivy4java integration**: Pure Java bindings, no Rust compilation needed
- **AWS support**: Full session token support for temporary credentials
- **Error handling**: Comprehensive validation with descriptive error messages
- **Performance**: Batch processing, predictive I/O, smart caching

---

**Instructions**: Do exactly what's asked, nothing more. Prefer editing existing files over creating new ones. Never create documentation files unless explicitly requested.